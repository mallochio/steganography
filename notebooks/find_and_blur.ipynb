{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "@File    :   Untitled-1\n",
    "@Time    :   2023/07/27 12:49:14\n",
    "@Author  :   Siddharth Ravi\n",
    "@Version :   1.0\n",
    "@Contact :   siddharth.ravi@ua.es\n",
    "@License :   (C)Copyright 2022-2023, Siddharth Ravi, Distributed under terms of the MIT license\n",
    "@Desc    :   Find and blur person in an image using a pretrained model with pytorch\n",
    "'''\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision.models.segmentation import DeepLabV3_ResNet101_Weights, deeplabv3_resnet101\n",
    "from torchvision.io.image import read_image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blur_person(img, mask):\n",
    "    # Change image from tensor to numpy array\n",
    "    img = img.numpy()\n",
    "    mask = mask.numpy()\n",
    "    print(img.shape, mask.shape)\n",
    "    masked_img = cv2.bitwise_and(img, mask)\n",
    "    cv2.imwrite(\"masked_img.jpg\", masked_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_person(image, model):\n",
    "    # Find the person in the image using deeplabv3_resnet101\n",
    "    weights = DeepLabV3_ResNet101_Weights.DEFAULT\n",
    "    model = deeplabv3_resnet101(weights=weights)\n",
    "    model.eval()\n",
    "\n",
    "    # Step 2: Initialize the inference transforms\n",
    "    preprocess = weights.transforms()\n",
    "\n",
    "    # Step 3: Apply inference preprocessing transforms\n",
    "    batch = preprocess(image).unsqueeze(0)\n",
    "\n",
    "    # Step 4: Use the model and visualize the prediction\n",
    "    prediction = model(batch)[\"out\"]\n",
    "    normalized_masks = prediction.softmax(dim=1)\n",
    "    class_to_idx = {cls: idx for (idx, cls) in enumerate(weights.meta[\"categories\"])}\n",
    "\n",
    "    mask = normalized_masks[0, class_to_idx[\"person\"]]\n",
    "    print(mask.shape, image.shape)\n",
    "    # Threshold the mask\n",
    "    mask = mask > 0.5\n",
    "    mask = mask.numpy().astype(np.uint8)\n",
    "    mask = mask * 255\n",
    "    \n",
    "    mask = torch.from_numpy(mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sid/mambaforge/envs/d2l/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/sid/mambaforge/envs/d2l/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([520, 924]) torch.Size([3, 720, 1280])\n",
      "(3, 720, 1280) (520, 924)\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) /Users/xperience/GHA-OpenCV-Python/_work/opencv-python/opencv-python/opencv/modules/core/src/arithm.cpp:214: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and type), nor 'array op scalar', nor 'scalar op array' in function 'binary_op'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m image \u001b[39m=\u001b[39m read_image(\u001b[39m\"\u001b[39m\u001b[39m1663943484522.jpg\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m mask \u001b[39m=\u001b[39m find_person(image, model)\n\u001b[0;32m----> 5\u001b[0m blur_person(image, mask)\n",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m, in \u001b[0;36mblur_person\u001b[0;34m(img, mask)\u001b[0m\n\u001b[1;32m      4\u001b[0m mask \u001b[39m=\u001b[39m mask\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(img\u001b[39m.\u001b[39mshape, mask\u001b[39m.\u001b[39mshape)\n\u001b[0;32m----> 6\u001b[0m masked_img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mbitwise_and(img, mask)\n\u001b[1;32m      7\u001b[0m cv2\u001b[39m.\u001b[39mimwrite(\u001b[39m\"\u001b[39m\u001b[39mmasked_img.jpg\u001b[39m\u001b[39m\"\u001b[39m, masked_img)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.8.0) /Users/xperience/GHA-OpenCV-Python/_work/opencv-python/opencv-python/opencv/modules/core/src/arithm.cpp:214: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and type), nor 'array op scalar', nor 'scalar op array' in function 'binary_op'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = torchvision.models.segmentation.deeplabv3_resnet101(pretrained=True)\n",
    "model.eval()\n",
    "image = read_image(\"1663943484522.jpg\")\n",
    "mask = find_person(image, model)\n",
    "blur_person(image, mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
